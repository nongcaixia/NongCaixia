# ç¬¬8ç« /streamlit_predict_v2.py - é€‚é…ä¸­æ–‡æ•°æ®é›†penguins-chinese.csv
import streamlit as st
import pickle
import pandas as pd
import os
import chardet
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# ===================== å…¨å±€é…ç½®ï¼ˆé€‚é…ä¸­æ–‡æ•°æ®é›†ï¼‰ =====================
DATA_PATH = "penguins-chinese.csv"  # ä¸­æ–‡æ•°æ®é›†è·¯å¾„
MODEL_PATH = "rfc_model.pkl"        # æ¨¡å‹ä¿å­˜è·¯å¾„
MAP_PATH = "output_uniques.pkl"     # ç‰©ç§æ˜ å°„æ–‡ä»¶è·¯å¾„
# ä¸­æ–‡æ•°æ®é›†å¿…è¦åˆ—åï¼ˆå·²ä»CSVè¯»å–ç¡®è®¤ï¼‰
REQUIRED_COLS = ["å²›å±¿", "æ€§åˆ«", "å–™é•¿åº¦(mm)", "å–™æ·±åº¦(mm)", "é³é•¿åº¦(mm)", "ä½“é‡(g)", "ç‰©ç§"]
# æ¨¡å‹è¾“å…¥ç‰¹å¾åˆ—åï¼ˆä¸­æ–‡ç¼–ç åï¼‰
FEATURE_NAMES = [
    "å–™é•¿åº¦(mm)", "å–™æ·±åº¦(mm)", "é³é•¿åº¦(mm)", "ä½“é‡(g)",
    "å²›å±¿_æ‰˜æ‰˜å°”æ£®å²›", "å²›å±¿_æ¯”æ–¯ç§‘å²›", "å²›å±¿_å¾·é‡Œå§†å²›",
    "æ€§åˆ«_é›Œæ€§", "æ€§åˆ«_é›„æ€§"
]

# ===================== è¾…åŠ©å‡½æ•°ï¼šæ£€æµ‹æ–‡ä»¶ç¼–ç  =====================
def detect_encoding(file_path):
    """æ£€æµ‹CSVæ–‡ä»¶ç¼–ç ï¼Œè§£å†³ä¸­æ–‡è§£ç é”™è¯¯"""
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)
    return chardet.detect(raw_data)['encoding']

# ===================== æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ =====================
def load_preprocess_data():
    """åŠ è½½ä¸­æ–‡æ•°æ®é›†å¹¶é¢„å¤„ç†"""
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATA_PATH):
        st.error(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ï¼š{DATA_PATH}ï¼Œè¯·æ”¾åœ¨ä»£ç åŒä¸€ç›®å½•ï¼")
        st.stop()
    
    # 2. è¯»å–ä¸­æ–‡CSVï¼ˆé€‚é…ç¼–ç ï¼‰
    try:
        df = pd.read_csv(DATA_PATH, encoding='gbk')
    except UnicodeDecodeError:
        enc = detect_encoding(DATA_PATH)
        st.warning(f"âš ï¸ ç”¨æ£€æµ‹åˆ°çš„ç¼–ç {enc}è¯»å–æ–‡ä»¶")
        df = pd.read_csv(DATA_PATH, encoding=enc)
    
    # 3. æ£€æŸ¥å¿…è¦åˆ—
    missing_cols = [col for col in REQUIRED_COLS if col not in df.columns]
    if missing_cols:
        st.error(f"âŒ æ•°æ®é›†ç¼ºå°‘åˆ—ï¼š{missing_cols}")
        st.error(f"å½“å‰åˆ—åï¼š{list(df.columns)}")
        st.stop()
    
    # 4. å¤„ç†ç¼ºå¤±å€¼
    df = df.dropna(subset=REQUIRED_COLS)
    if len(df) == 0:
        st.error("âŒ æ•°æ®é›†æ— æœ‰æ•ˆæ•°æ®ï¼ˆå…¨ä¸ºç¼ºå¤±å€¼ï¼‰")
        st.stop()
    
    # 5. åˆ†ç¦»ç‰¹å¾ï¼ˆXï¼‰å’Œæ ‡ç­¾ï¼ˆyï¼‰
    X = df[["å²›å±¿", "æ€§åˆ«", "å–™é•¿åº¦(mm)", "å–™æ·±åº¦(mm)", "é³é•¿åº¦(mm)", "ä½“é‡(g)"]]
    y = df["ç‰©ç§"]
    
    # 6. åˆ†ç±»ç‰¹å¾ç‹¬çƒ­ç¼–ç ï¼ˆä¸­æ–‡å€¼ï¼‰
    cat_encoder = OneHotEncoder(sparse_output=False, drop=None)
    encoded_cats = cat_encoder.fit_transform(X[["å²›å±¿", "æ€§åˆ«"]])
    
    # 7. æ„é€ ç¼–ç åç‰¹å¾å
    encoded_names = []
    for i, feat in enumerate(["å²›å±¿", "æ€§åˆ«"]):
        for cat in cat_encoder.categories_[i]:
            encoded_names.append(f"{feat}_{cat}")
    
    # 8. åˆå¹¶æ•°å€¼ç‰¹å¾ä¸ç¼–ç ç‰¹å¾
    numeric_feat = X[["å–™é•¿åº¦(mm)", "å–™æ·±åº¦(mm)", "é³é•¿åº¦(mm)", "ä½“é‡(g)"]].reset_index(drop=True)
    encoded_df = pd.DataFrame(encoded_cats, columns=encoded_names)
    X_processed = pd.concat([numeric_feat, encoded_df], axis=1)
    
    # 9. è¡¥å…¨ç‰¹å¾åˆ—ï¼ˆç¡®ä¿ä¸FEATURE_NAMESä¸€è‡´ï¼‰
    for col in FEATURE_NAMES:
        if col not in X_processed.columns:
            X_processed[col] = 0
    X_processed = X_processed[FEATURE_NAMES]
    
    # 10. ç”Ÿæˆç‰©ç§æ˜ å°„ï¼ˆç”¨äºé¢„æµ‹ç»“æœæ˜¾ç¤ºï¼‰
    species_map = {i: sp for i, sp in enumerate(y.unique())}
    return X_processed, y, species_map, cat_encoder

def train_model():
    """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹å¹¶ä¿å­˜ï¼ˆæ— æ¨¡å‹æ—¶è‡ªåŠ¨æ‰§è¡Œï¼‰"""
    if os.path.exists(MODEL_PATH) and os.path.exists(MAP_PATH):
        return
    
    # åŠ è½½é¢„å¤„ç†æ•°æ®
    X, y, species_map, _ = load_preprocess_data()
    
    # åˆ’åˆ†è®­ç»ƒé›†
    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    rfc = RandomForestClassifier(n_estimators=100, random_state=42)
    rfc.fit(X_train, y_train)
    
    # ä¿å­˜æ¨¡å‹å’Œæ˜ å°„
    with open(MODEL_PATH, 'wb') as f:
        pickle.dump(rfc, f)
    with open(MAP_PATH, 'wb') as f:
        pickle.dump(species_map, f)
    
    st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼å·²ç”Ÿæˆrfc_model.pklå’Œoutput_uniques.pkl")

# ===================== é¡µé¢åŠŸèƒ½ =====================
def intro_page():
    """ç®€ä»‹é¡µé¢ï¼ˆé€‚é…ä¸­æ–‡æ•°æ®é›†è¯´æ˜ï¼‰"""
    st.title("ä¼é¹…åˆ†ç±»å™¨ ğŸ§")
    st.header("æ•°æ®é›†ä»‹ç»ï¼ˆä¸­æ–‡ç‰ˆï¼‰")
    st.markdown("""**å¸•å°”é»˜ç¾¤å²›ä¼é¹…ä¸­æ–‡æ•°æ®é›†åŒ…å«344æ¡è§‚æµ‹è®°å½•ï¼Œæ¶µç›–3ç§å—æä¼é¹…ï¼š
    é˜¿å¾·åˆ©ä¼é¹…ã€å·´å¸ƒäºšä¼é¹…å’Œå¸½å¸¦ä¼é¹…ã€‚æ•°æ®è®°å½•äº†ä¼é¹…çš„æ –æ¯å²›å±¿ã€æ€§åˆ«ã€
    å–™é•¿åº¦/æ·±åº¦ã€é³é•¿åº¦åŠä½“é‡ç­‰å…³é”®ç‰¹å¾ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ åˆ†ç±»ç»ƒä¹ å’Œæ•°æ®å¯è§†åŒ–åˆ†æã€‚**""")
    
    st.header("ä¸‰ç§ä¼é¹…ç‰¹å¾å·®å¼‚")
    st.markdown("""
    - **é˜¿å¾·åˆ©ä¼é¹…**ï¼šä½“å‹è¾ƒå°ï¼Œå–™çŸ­è€Œé’ï¼Œä¸»è¦æ –æ¯äºæ‰˜æ‰˜å°”æ£®å²›ï¼›
    - **å·´å¸ƒäºšä¼é¹…**ï¼šä½“å‹è¾ƒå¤§ï¼Œå–™å°–ä¸”é•¿ï¼Œé³é•¿åº¦æœ€é•¿ï¼›
    - **å¸½å¸¦ä¼é¹…**ï¼šå¤´éƒ¨æœ‰é»‘è‰²æ¡çº¹ï¼ˆä¼¼å¸½å¸¦ï¼‰ï¼Œå–™ä¸­ç­‰é•¿åº¦ã€‚
    """)
    
    # è‹¥æ²¡æœ‰imagesæ–‡ä»¶å¤¹ï¼Œå¯æ³¨é‡Šä¸‹æ–¹å›¾ç‰‡ä»£ç ï¼Œé¿å…æŠ¥é”™
    try:
        st.image('images/penguins.png', caption="ä¸‰ç§ä¼é¹…å¡é€šå›¾")
    except:
        st.info("âš ï¸ æœªæ‰¾åˆ°images/penguins.pngï¼Œå¯è‡ªè¡Œæ·»åŠ å›¾ç‰‡æ–‡ä»¶å±•ç¤ºä¼é¹…ç¤ºæ„å›¾")

def predict_page():
    """é¢„æµ‹é¡µé¢ï¼ˆå…¨ä¸­æ–‡äº¤äº’ï¼‰"""
    st.header("ä¼é¹…ç‰©ç§é¢„æµ‹")
    st.markdown("""è¾“å…¥ä»¥ä¸‹6é¡¹ä¼é¹…ç‰¹å¾ï¼Œç³»ç»Ÿå°†åŸºäºéšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å…¶ç‰©ç§ï¼š
    - æ³¨ï¼šç‰¹å¾å€¼éœ€ç¬¦åˆå®é™…èŒƒå›´ï¼ˆå¦‚å–™é•¿åº¦30-60mmï¼Œä½“é‡2700-6300gï¼‰""")
    
    # 3:1:2åˆ—å¸ƒå±€
    col_form, col1, col_logo = st.columns([3, 1, 2])
    with col_form:
        with st.form('user_inputs'):
            # ä¸­æ–‡è¾“å…¥è¡¨å•ï¼ˆä¸æ•°æ®é›†åˆ†ç±»å€¼ä¸€è‡´ï¼‰
            island = st.selectbox('æ –æ¯å²›å±¿', options=['æ‰˜æ‰˜å°”æ£®å²›', 'æ¯”æ–¯ç§‘å²›', 'å¾·é‡Œå§†å²›'])
            sex = st.selectbox('æ€§åˆ«', options=['é›Œæ€§', 'é›„æ€§'])
            bill_length = st.number_input('å–™é•¿åº¦(æ¯«ç±³)', min_value=30.0, max_value=60.0, value=45.0)
            bill_depth = st.number_input('å–™æ·±åº¦(æ¯«ç±³)', min_value=15.0, max_value=25.0, value=20.0)
            flipper_length = st.number_input('é³é•¿åº¦(æ¯«ç±³)', min_value=170.0, max_value=240.0, value=200.0)
            body_mass = st.number_input('ä½“é‡(å…‹)', min_value=2700.0, max_value=6300.0, value=4000.0)
            submitted = st.form_submit_button('é¢„æµ‹ç‰©ç§', type='primary')

            # 1. åˆå§‹åŒ–ç‰¹å¾å‘é‡ï¼ˆä¸FEATURE_NAMESå¯¹é½ï¼‰
            feature_vec = [0] * len(FEATURE_NAMES)
            # å¡«å……æ•°å€¼ç‰¹å¾
            feature_vec[FEATURE_NAMES.index("å–™é•¿åº¦(mm)")] = bill_length
            feature_vec[FEATURE_NAMES.index("å–™æ·±åº¦(mm)")] = bill_depth
            feature_vec[FEATURE_NAMES.index("é³é•¿åº¦(mm)")] = flipper_length
            feature_vec[FEATURE_NAMES.index("ä½“é‡(g)")] = body_mass
            # å¡«å……åˆ†ç±»ç‰¹å¾ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰
            feature_vec[FEATURE_NAMES.index(f"å²›å±¿_{island}")] = 1
            feature_vec[FEATURE_NAMES.index(f"æ€§åˆ«_{sex}")] = 1

            # 2. åŠ è½½æ¨¡å‹ï¼ˆæ— æ¨¡å‹åˆ™è‡ªåŠ¨è®­ç»ƒï¼‰
            if not (os.path.exists(MODEL_PATH) and os.path.exists(MAP_PATH)):
                with st.spinner("é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨è®­ç»ƒæ¨¡å‹...ï¼ˆçº¦5ç§’ï¼‰"):
                    train_model()

            # 3. é¢„æµ‹é€»è¾‘
            if submitted:
                try:
                    # åŠ è½½æ¨¡å‹å’Œç‰©ç§æ˜ å°„
                    with open(MODEL_PATH, 'rb') as f:
                        rfc_model = pickle.load(f)
                    with open(MAP_PATH, 'rb') as f:
                        species_map = pickle.load(f)

                    # æ ¼å¼åŒ–è¾“å…¥æ•°æ®
                    input_df = pd.DataFrame([feature_vec], columns=FEATURE_NAMES)
                    # é¢„æµ‹ï¼ˆè¿”å›ç‰©ç§ç´¢å¼•ï¼Œæ˜ å°„ä¸ºä¸­æ–‡åç§°ï¼‰
                    pred_idx = rfc_model.predict(input_df)[0]
                    pred_species = species_map[pred_idx]

                    # æ˜¾ç¤ºç»“æœ
                    st.success(f"ğŸ‰ é¢„æµ‹ç»“æœï¼šè¯¥ä¼é¹…ä¸º **{pred_species}**")

                except Exception as e:
                    st.error(f"âŒ é¢„æµ‹å‡ºé”™ï¼š{str(e)}")
                    st.info("å»ºè®®ï¼šåˆ é™¤rfc_model.pklå’Œoutput_uniques.pklåé‡æ–°è¿è¡Œï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹")

    # å³ä¾§æ˜¾ç¤ºå›¾ç‰‡ï¼ˆæ— å›¾ç‰‡æ—¶æ˜¾ç¤ºæç¤ºï¼‰
    with col_logo:
        if not submitted:
            try:
                st.image('images/rigth_logo.png', width=300, caption="ä¼é¹…åˆ†ç±»å™¨")
            except:
                st.info("âš ï¸ æœªæ‰¾åˆ°images/rigth_logo.pngï¼Œå¯æ·»åŠ Logoå›¾ç‰‡")
        else:
            # é¢„æµ‹åæ˜¾ç¤ºå¯¹åº”ç‰©ç§å›¾ç‰‡ï¼ˆéœ€æå‰å‡†å¤‡å›¾ç‰‡ï¼Œå‘½åä¸ºç‰©ç§å.pngï¼‰
            try:
                st.image(f'images/{pred_species}.png', width=300, caption=f"{pred_species}")
            except:
                st.info(f"âš ï¸ æœªæ‰¾åˆ°{pred_species}çš„å›¾ç‰‡ï¼Œå¯æ·»åŠ images/{pred_species}.png")

# ===================== ä¸»ç¨‹åº =====================
def main():
    # é¡µé¢åŸºç¡€é…ç½®
    st.set_page_config(
        page_title="ä¼é¹…åˆ†ç±»å™¨ï¼ˆä¸­æ–‡æ•°æ®é›†ç‰ˆï¼‰",
        page_icon="ğŸ§",
        layout="wide"
    )

    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        # æ— logoå›¾ç‰‡æ—¶æ³¨é‡Šä¸‹æ–¹ä»£ç 
        try:
            st.image('images/rigth_logo.png', width=100)
        except:
            pass
        st.title('åŠŸèƒ½å¯¼èˆª')
        page = st.selectbox(
            "é€‰æ‹©é¡µé¢", 
            ["ç®€ä»‹é¡µé¢", "é¢„æµ‹åˆ†ç±»é¡µé¢"], 
            label_visibility='collapsed'
        )

    # é¡µé¢è·¯ç”±
    if page == "ç®€ä»‹é¡µé¢":
        intro_page()
    else:
        predict_page()

if __name__ == "__main__":
    main()
