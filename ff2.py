# ç¬¬8ç« /streamlit_predict_v2.py - ç›´æ¥è¯»å–æ ¹ç›®å½•å›¾ç‰‡ï¼ˆé€‚é…ä½ çš„æ–‡ä»¶ç»“æ„ï¼‰
import streamlit as st
import pickle
import pandas as pd
import os
import chardet
from PIL import Image
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# ===================== å…¨å±€é…ç½®ï¼ˆé€‚é…æ ¹ç›®å½•å›¾ç‰‡ï¼‰ =====================
DATA_PATH = "penguins-chinese.csv"  # ä¸­æ–‡æ•°æ®é›†è·¯å¾„
MODEL_PATH = "rfc_model.pkl"        # æ¨¡å‹ä¿å­˜è·¯å¾„
MAP_PATH = "output_uniques.pkl"     # ç‰©ç§æ˜ å°„æ–‡ä»¶è·¯å¾„
# å®é™…æ•°æ®é›†å¿…è¦åˆ—å
REQUIRED_COLS = [
    "ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«", "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", 
    "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡", "ä¼é¹…çš„ç§ç±»"
]
# æ¨¡å‹è¾“å…¥ç‰¹å¾åˆ—å
FEATURE_NAMES = [
    "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡",
    "ä¼é¹…æ –æ¯çš„å²›å±¿_æ‰˜æ‰˜å°”æ£®å²›", "ä¼é¹…æ –æ¯çš„å²›å±¿_æ¯”æ–¯ç§‘å²›", "ä¼é¹…æ –æ¯çš„å²›å±¿_å¾·é‡Œå§†å²›",
    "æ€§åˆ«_é›Œæ€§", "æ€§åˆ«_é›„æ€§"
]

# ===================== æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šç›´æ¥è¯»å–æ ¹ç›®å½•å›¾ç‰‡ =====================
def load_local_image(image_filename):
    """è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å›¾ç‰‡ï¼ˆé€‚é…ä½ çš„æ–‡ä»¶ç»“æ„ï¼‰"""
    try:
        # å›¾ç‰‡ç›´æ¥åœ¨æ ¹ç›®å½•ï¼Œè·¯å¾„å°±æ˜¯æ–‡ä»¶åæœ¬èº«
        if os.path.exists(image_filename):
            return Image.open(image_filename)
        else:
            st.warning(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡ï¼š{image_filename}ï¼ˆè¯·ç¡®è®¤æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰")
            return None
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å–å›¾ç‰‡å¤±è´¥ï¼š{str(e)}")
        return None

# ===================== è¾…åŠ©å‡½æ•°ï¼šæ£€æµ‹æ–‡ä»¶ç¼–ç  =====================
def detect_encoding(file_path):
    """æ£€æµ‹CSVæ–‡ä»¶ç¼–ç """
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)
    return chardet.detect(raw_data)['encoding']

# ===================== æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ =====================
def load_preprocess_data():
    """åŠ è½½æ•°æ®é›†å¹¶é¢„å¤„ç†"""
    if not os.path.exists(DATA_PATH):
        st.error(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ï¼š{DATA_PATH}ï¼ˆè¯·æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰")
        st.stop()
    
    # è¯»å–ä¸­æ–‡CSVï¼ˆé€‚é…ç¼–ç ï¼‰
    try:
        df = pd.read_csv(DATA_PATH, encoding='gbk')
    except UnicodeDecodeError:
        enc = detect_encoding(DATA_PATH)
        st.warning(f"âš ï¸ ç”¨ç¼–ç {enc}è¯»å–æ–‡ä»¶")
        df = pd.read_csv(DATA_PATH, encoding=enc)
    
    # æ£€æŸ¥å¿…è¦åˆ—
    missing_cols = [col for col in REQUIRED_COLS if col not in df.columns]
    if missing_cols:
        st.error(f"âŒ æ•°æ®é›†ç¼ºå°‘åˆ—ï¼š{missing_cols}")
        st.stop()
    
    # å¤„ç†ç¼ºå¤±å€¼
    df = df.dropna(subset=REQUIRED_COLS)
    if len(df) == 0:
        st.error("âŒ æ•°æ®é›†æ— æœ‰æ•ˆæ•°æ®")
        st.stop()
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = df[["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«", "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]]
    y = df["ä¼é¹…çš„ç§ç±»"]
    
    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    species_map = {i: sp for i, sp in enumerate(label_encoder.classes_)}
    
    # ç‹¬çƒ­ç¼–ç åˆ†ç±»ç‰¹å¾
    cat_encoder = OneHotEncoder(sparse_output=False, drop=None)
    encoded_cats = cat_encoder.fit_transform(X[["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«"]])
    
    # æ„é€ ç¼–ç ç‰¹å¾å
    encoded_names = []
    for i, feat in enumerate(["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«"]):
        for cat in cat_encoder.categories_[i]:
            encoded_names.append(f"{feat}_{cat}")
    
    # åˆå¹¶ç‰¹å¾
    numeric_feat = X[["å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]].reset_index(drop=True)
    encoded_df = pd.DataFrame(encoded_cats, columns=encoded_names)
    X_processed = pd.concat([numeric_feat, encoded_df], axis=1)
    
    # è¡¥å…¨ç‰¹å¾åˆ—
    for col in FEATURE_NAMES:
        if col not in X_processed.columns:
            X_processed[col] = 0.0
    X_processed = X_processed[FEATURE_NAMES]
    
    return X_processed, y_encoded, species_map, cat_encoder, label_encoder

def train_model(force_retrain=False):
    """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹"""
    if force_retrain:
        # å¼ºåˆ¶åˆ é™¤æ—§æ¨¡å‹
        if os.path.exists(MODEL_PATH):
            os.remove(MODEL_PATH)
        if os.path.exists(MAP_PATH):
            os.remove(MAP_PATH)
    
    # å·²æœ‰æ¨¡å‹åˆ™è·³è¿‡
    if os.path.exists(MODEL_PATH) and os.path.exists(MAP_PATH) and not force_retrain:
        return
    
    # åŠ è½½æ•°æ®å¹¶è®­ç»ƒ
    X, y, species_map, _, _ = load_preprocess_data()
    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)
    rfc = RandomForestClassifier(n_estimators=100, random_state=42)
    rfc.fit(X_train, y_train)
    
    # ä¿å­˜æ¨¡å‹
    with open(MODEL_PATH, 'wb') as f:
        pickle.dump(rfc, f)
    with open(MAP_PATH, 'wb') as f:
        pickle.dump(species_map, f)
    
    st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

# ===================== é¡µé¢åŠŸèƒ½ï¼ˆç›´æ¥è¯»å–æ ¹ç›®å½•å›¾ç‰‡ï¼‰ =====================
def intro_page():
    """ç®€ä»‹é¡µé¢"""
    st.title("ä¼é¹…åˆ†ç±»å™¨ ğŸ§")
    st.header("æ•°æ®é›†ä»‹ç»ï¼ˆä¸­æ–‡ç‰ˆï¼‰")
    st.markdown("""**å¸•å°”é»˜ç¾¤å²›ä¼é¹…ä¸­æ–‡æ•°æ®é›†åŒ…å«344æ¡è§‚æµ‹è®°å½•ï¼Œæ¶µç›–3ç§å—æä¼é¹…ï¼š
    é˜¿å¾·åˆ©ä¼é¹…ã€å·´å¸ƒäºšä¼é¹…å’Œå¸½å¸¦ä¼é¹…ã€‚**""")
    
    st.header("ä¸‰ç§ä¼é¹…ç‰¹å¾å·®å¼‚")
    st.markdown("""
    - **é˜¿å¾·åˆ©ä¼é¹…**ï¼šä½“å‹è¾ƒå°ï¼Œå–™çŸ­è€Œé’ï¼›
    - **å·´å¸ƒäºšä¼é¹…**ï¼šä½“å‹è¾ƒå¤§ï¼Œå–™å°–ä¸”é•¿ï¼›
    - **å¸½å¸¦ä¼é¹…**ï¼šå¤´éƒ¨æœ‰é»‘è‰²æ¡çº¹ï¼Œå–™ä¸­ç­‰é•¿åº¦ã€‚
    """)
    
    # è¯»å–æ ¹ç›®å½•çš„ã€Œpenguins.pngã€ï¼ˆä½ çš„æ–‡ä»¶é‡Œçš„å›¾ç‰‡ï¼‰
    penguin_img = load_local_image("penguins.png")
    if penguin_img:
        st.image(penguin_img, caption="ä¸‰ç§ä¼é¹…å¡é€šå›¾")
    else:
        st.info("âš ï¸ æœªåŠ è½½åˆ°ä¼é¹…ç¤ºæ„å›¾")

def predict_page():
    """é¢„æµ‹é¡µé¢ï¼ˆé€‚é…æ ¹ç›®å½•å›¾ç‰‡ï¼‰"""
    st.header("ä¼é¹…ç‰©ç§é¢„æµ‹")
    st.markdown("""è¾“å…¥ä¼é¹…ç‰¹å¾ï¼Œç³»ç»Ÿå°†é¢„æµ‹å…¶ç‰©ç§ï¼š
    - æ³¨ï¼šç‰¹å¾å€¼éœ€ç¬¦åˆå®é™…èŒƒå›´ï¼ˆå¦‚å–™é•¿åº¦30-60mmï¼Œèº«ä½“è´¨é‡2700-6300gï¼‰""")
    
    # å¸ƒå±€
    col_form, col1, col_logo = st.columns([3, 1, 2])
    with col_form:
        with st.form('user_inputs'):
            # è¾“å…¥è¡¨å•
            island = st.selectbox('æ –æ¯å²›å±¿', options=['æ‰˜æ‰˜å°”æ£®å²›', 'æ¯”æ–¯ç§‘å²›', 'å¾·é‡Œå§†å²›'])
            sex = st.selectbox('æ€§åˆ«', options=['é›Œæ€§', 'é›„æ€§'])
            bill_length = st.number_input('å–™çš„é•¿åº¦(æ¯«ç±³)', min_value=30.0, max_value=60.0, value=45.0)
            bill_depth = st.number_input('å–™çš„æ·±åº¦(æ¯«ç±³)', min_value=15.0, max_value=25.0, value=20.0)
            flipper_length = st.number_input('ç¿…è†€çš„é•¿åº¦(æ¯«ç±³)', min_value=170.0, max_value=240.0, value=200.0)
            body_mass = st.number_input('èº«ä½“è´¨é‡(å…‹)', min_value=2700.0, max_value=6300.0, value=4000.0)
            submitted = st.form_submit_button('é¢„æµ‹ç‰©ç§', type='primary')

            # åˆå§‹åŒ–ç‰¹å¾å‘é‡
            feature_vec = [0.0] * len(FEATURE_NAMES)
            # å¡«å……æ•°å€¼ç‰¹å¾
            feature_vec[FEATURE_NAMES.index("å–™çš„é•¿åº¦")] = bill_length
            feature_vec[FEATURE_NAMES.index("å–™çš„æ·±åº¦")] = bill_depth
            feature_vec[FEATURE_NAMES.index("ç¿…è†€çš„é•¿åº¦")] = flipper_length
            feature_vec[FEATURE_NAMES.index("èº«ä½“è´¨é‡")] = body_mass
            # å¡«å……åˆ†ç±»ç‰¹å¾
            feature_vec[FEATURE_NAMES.index(f"ä¼é¹…æ –æ¯çš„å²›å±¿_{island}")] = 1.0
            feature_vec[FEATURE_NAMES.index(f"æ€§åˆ«_{sex}")] = 1.0

            # é¢„æµ‹é€»è¾‘
            pred_species = ""
            if submitted:
                try:
                    train_model(force_retrain=True)

                    # åŠ è½½æ¨¡å‹
                    with open(MODEL_PATH, 'rb') as f:
                        rfc_model = pickle.load(f)
                    with open(MAP_PATH, 'rb') as f:
                        species_map = pickle.load(f)

                    # é¢„æµ‹
                    input_df = pd.DataFrame([feature_vec], columns=FEATURE_NAMES, dtype=float)
                    pred_idx = rfc_model.predict(input_df)[0]
                    pred_species = species_map.get(int(pred_idx), "æœªçŸ¥ç‰©ç§")

                    # æ˜¾ç¤ºç»“æœ
                    if pred_species != "æœªçŸ¥ç‰©ç§":
                        st.success(f"ğŸ‰ é¢„æµ‹ç»“æœï¼šè¯¥ä¼é¹…ä¸º **{pred_species}**")
                    else:
                        st.warning("âš ï¸ æ— æ³•è¯†åˆ«è¯¥ä¼é¹…ç‰©ç§")

                except Exception as e:
                    st.error(f"âŒ é¢„æµ‹å‡ºé”™ï¼š{str(e)}")
                    st.info("å·²è‡ªåŠ¨é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè¯·å†æ¬¡ç‚¹å‡»é¢„æµ‹")
                    train_model(force_retrain=True)

    # å³ä¾§å›¾ç‰‡åŒºåŸŸï¼ˆç›´æ¥è¯»å–æ ¹ç›®å½•å›¾ç‰‡ï¼‰
    with col_logo:
        if not submitted:
            # è¯»å–æ ¹ç›®å½•çš„ã€Œrigth_logo.pngã€ï¼ˆä½ çš„æ–‡ä»¶é‡Œçš„logoï¼‰
            logo_img = load_local_image("rigth_logo.png")
            if logo_img:
                st.image(logo_img, width=300, caption="ä¼é¹…åˆ†ç±»å™¨")
            else:
                st.info("âš ï¸ æœªåŠ è½½åˆ°Logoå›¾ç‰‡")
        else:
            # è¯»å–æ ¹ç›®å½•çš„ç‰©ç§å›¾ç‰‡ï¼ˆä½ çš„æ–‡ä»¶é‡Œçš„ï¼šé˜¿å¾·åˆ©ä¼é¹….pngã€å·´å¸ƒäºšä¼é¹….pngã€å¸½å¸¦ä¼é¹….pngï¼‰
            if pred_species and pred_species != "æœªçŸ¥ç‰©ç§":
                species_img = load_local_image(f"{pred_species}.png")
                if species_img:
                    st.image(species_img, width=300, caption=f"{pred_species}")
                else:
                    st.info(f"âš ï¸ æœªåŠ è½½åˆ°{pred_species}çš„å›¾ç‰‡")
            else:
                st.info("âš ï¸ æš‚æ— æœ‰æ•ˆé¢„æµ‹ç»“æœ")

# ===================== ä¸»ç¨‹åº =====================
def main():
    st.set_page_config(
        page_title="ä¼é¹…åˆ†ç±»å™¨ï¼ˆä¸­æ–‡æ•°æ®é›†ç‰ˆï¼‰",
        page_icon="ğŸ§",
        layout="wide"
    )

    # ä¾§è¾¹æ 
    with st.sidebar:
        # è¯»å–æ ¹ç›®å½•çš„Logo
        sidebar_logo = load_local_image("rigth_logo.png")
        if sidebar_logo:
            st.image(sidebar_logo, width=100)
        st.title('åŠŸèƒ½å¯¼èˆª')
        page = st.selectbox(
            "é€‰æ‹©é¡µé¢", 
            ["ç®€ä»‹é¡µé¢", "é¢„æµ‹åˆ†ç±»é¡µé¢"], 
            label_visibility='collapsed'
        )

    # é¡µé¢è·¯ç”±
    if page == "ç®€ä»‹é¡µé¢":
        intro_page()
    else:
        predict_page()

if __name__ == "__main__":
    main()
