# ç¬¬8ç« /streamlit_predict_v2.py - ä¿®å¤ç‰©ç§æ˜ å°„+ç‰¹å¾åˆ—åé—®é¢˜
import streamlit as st
import pickle
import pandas as pd
import os
import chardet
import zipfile
import io
from PIL import Image
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# ===================== å…¨å±€é…ç½®ï¼ˆé€‚é…å®é™…æ•°æ®é›†åˆ—åï¼‰ =====================
DATA_PATH = "penguins-chinese.csv"  # ä¸­æ–‡æ•°æ®é›†è·¯å¾„
MODEL_PATH = "rfc_model.pkl"        # æ¨¡å‹ä¿å­˜è·¯å¾„
MAP_PATH = "output_uniques.pkl"     # ç‰©ç§æ˜ å°„æ–‡ä»¶è·¯å¾„
ZIP_IMAGE_PATH = "images.zip"       # å›¾ç‰‡å‹ç¼©åŒ…è·¯å¾„ï¼ˆä¸ä»£ç åŒç›®å½•ï¼‰
# å®é™…æ•°æ®é›†å¿…è¦åˆ—åï¼ˆä»æŠ¥é”™ä¿¡æ¯ä¸­æå–ï¼‰
REQUIRED_COLS = [
    "ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«", "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", 
    "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡", "ä¼é¹…çš„ç§ç±»"
]
# æ¨¡å‹è¾“å…¥ç‰¹å¾åˆ—åï¼ˆé€‚é…å®é™…åˆ†ç±»ç‰¹å¾ï¼‰
FEATURE_NAMES = [
    "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡",
    "ä¼é¹…æ –æ¯çš„å²›å±¿_æ‰˜æ‰˜å°”æ£®å²›", "ä¼é¹…æ –æ¯çš„å²›å±¿_æ¯”æ–¯ç§‘å²›", "ä¼é¹…æ –æ¯çš„å²›å±¿_å¾·é‡Œå§†å²›",
    "æ€§åˆ«_é›Œæ€§", "æ€§åˆ«_é›„æ€§"
]

# ===================== æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè¯»å–ZIPå†…å›¾ç‰‡ï¼ˆé€‚é…zipæ ¹ç›®å½•ï¼‰ =====================
def load_image_from_zip(zip_file_path, image_filename):
    """ä»zipå‹ç¼©åŒ…æ ¹ç›®å½•è¯»å–å›¾ç‰‡"""
    try:
        if not os.path.exists(zip_file_path):
            st.warning(f"âŒ å›¾ç‰‡å‹ç¼©åŒ… {zip_file_path} æœªæ‰¾åˆ°ï¼")
            return None
        
        with zipfile.ZipFile(zip_file_path, 'r') as zf:
            if image_filename not in zf.namelist():
                st.warning(f"âŒ zipå†…æœªæ‰¾åˆ°å›¾ç‰‡ï¼š{image_filename}")
                st.info(f"zipå†…æ‰€æœ‰æ–‡ä»¶ï¼š{zf.namelist()[:5]}...")
                return None
            
            with zf.open(image_filename) as f:
                img_data = io.BytesIO(f.read())
                img = Image.open(img_data)
                return img
    except Exception as e:
        st.warning(f"âš ï¸ è¯»å–å›¾ç‰‡å¤±è´¥ï¼š{str(e)}")
        return None

# ===================== è¾…åŠ©å‡½æ•°ï¼šæ£€æµ‹æ–‡ä»¶ç¼–ç  =====================
def detect_encoding(file_path):
    """æ£€æµ‹CSVæ–‡ä»¶ç¼–ç ï¼Œè§£å†³ä¸­æ–‡è§£ç é”™è¯¯"""
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)
    return chardet.detect(raw_data)['encoding']

# ===================== æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒï¼ˆé€‚é…å®é™…åˆ—åï¼‰ =====================
def load_preprocess_data():
    """åŠ è½½å®é™…æ•°æ®é›†å¹¶é¢„å¤„ç†ï¼ˆä¿®å¤ç‰©ç§æ˜ å°„ï¼‰"""
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATA_PATH):
        st.error(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ï¼š{DATA_PATH}ï¼Œè¯·æ”¾åœ¨ä»£ç åŒä¸€ç›®å½•ï¼")
        st.stop()
    
    # 2. è¯»å–ä¸­æ–‡CSVï¼ˆé€‚é…ç¼–ç ï¼‰
    try:
        df = pd.read_csv(DATA_PATH, encoding='gbk')
    except UnicodeDecodeError:
        enc = detect_encoding(DATA_PATH)
        st.warning(f"âš ï¸ ç”¨æ£€æµ‹åˆ°çš„ç¼–ç {enc}è¯»å–æ–‡ä»¶")
        df = pd.read_csv(DATA_PATH, encoding=enc)
    
    # 3. æ£€æŸ¥å¿…è¦åˆ—
    missing_cols = [col for col in REQUIRED_COLS if col not in df.columns]
    if missing_cols:
        st.error(f"âŒ æ•°æ®é›†ç¼ºå°‘åˆ—ï¼š{missing_cols}")
        st.error(f"å½“å‰åˆ—åï¼š{list(df.columns)}")
        st.stop()
    
    # 4. å¤„ç†ç¼ºå¤±å€¼
    df = df.dropna(subset=REQUIRED_COLS)
    if len(df) == 0:
        st.error("âŒ æ•°æ®é›†æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼ˆå…¨ä¸ºç¼ºå¤±å€¼ï¼‰")
        st.stop()
    
    # 5. åˆ†ç¦»ç‰¹å¾ï¼ˆXï¼‰å’Œæ ‡ç­¾ï¼ˆyï¼‰ï¼ˆç”¨å®é™…åˆ—åï¼‰
    X = df[["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«", "å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]]
    y = df["ä¼é¹…çš„ç§ç±»"]
    
    # 6. æ ‡ç­¾ç¼–ç ï¼ˆç¡®ä¿ç‰©ç§ç´¢å¼•ä¸æ¨¡å‹ä¸€è‡´ï¼‰
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    # ä¿å­˜ç‰©ç§æ˜ å°„ï¼ˆç”¨LabelEncoderçš„classes_ï¼Œç¡®ä¿ç´¢å¼•æ­£ç¡®ï¼‰
    species_map = {i: sp for i, sp in enumerate(label_encoder.classes_)}
    
    # 7. åˆ†ç±»ç‰¹å¾ç‹¬çƒ­ç¼–ç ï¼ˆé€‚é…å®é™…åˆ—åï¼‰
    cat_encoder = OneHotEncoder(sparse_output=False, drop=None)
    encoded_cats = cat_encoder.fit_transform(X[["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«"]])
    
    # 8. æ„é€ ç¼–ç åç‰¹å¾åï¼ˆé€‚é…å®é™…åˆ—åï¼‰
    encoded_names = []
    for i, feat in enumerate(["ä¼é¹…æ –æ¯çš„å²›å±¿", "æ€§åˆ«"]):
        for cat in cat_encoder.categories_[i]:
            encoded_names.append(f"{feat}_{cat}")
    
    # 9. åˆå¹¶æ•°å€¼ç‰¹å¾ä¸ç¼–ç ç‰¹å¾
    numeric_feat = X[["å–™çš„é•¿åº¦", "å–™çš„æ·±åº¦", "ç¿…è†€çš„é•¿åº¦", "èº«ä½“è´¨é‡"]].reset_index(drop=True)
    encoded_df = pd.DataFrame(encoded_cats, columns=encoded_names)
    X_processed = pd.concat([numeric_feat, encoded_df], axis=1)
    
    # 10. è¡¥å…¨ç‰¹å¾åˆ—ï¼ˆç¡®ä¿ä¸FEATURE_NAMESä¸€è‡´ï¼‰
    for col in FEATURE_NAMES:
        if col not in X_processed.columns:
            X_processed[col] = 0.0
    X_processed = X_processed[FEATURE_NAMES]
    
    return X_processed, y_encoded, species_map, cat_encoder, label_encoder

def train_model(force_retrain=False):
    """
    è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹å¹¶ä¿å­˜
    :param force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼ˆåˆ é™¤æ—§æ¨¡å‹ï¼‰
    """
    # å¼ºåˆ¶åˆ é™¤æ—§æ¨¡å‹
    if force_retrain:
        if os.path.exists(MODEL_PATH):
            os.remove(MODEL_PATH)
        if os.path.exists(MAP_PATH):
            os.remove(MAP_PATH)
    
    # å·²æœ‰æ¨¡å‹ä¸”ä¸å¼ºåˆ¶é‡è®­åˆ™ç›´æ¥è¿”å›
    if os.path.exists(MODEL_PATH) and os.path.exists(MAP_PATH) and not force_retrain:
        return
    
    # åŠ è½½é¢„å¤„ç†æ•°æ®
    X, y, species_map, _, _ = load_preprocess_data()
    
    # åˆ’åˆ†è®­ç»ƒé›†
    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    rfc = RandomForestClassifier(n_estimators=100, random_state=42)
    rfc.fit(X_train, y_train)
    
    # ä¿å­˜æ¨¡å‹å’Œç‰©ç§æ˜ å°„
    with open(MODEL_PATH, 'wb') as f:
        pickle.dump(rfc, f)
    with open(MAP_PATH, 'wb') as f:
        pickle.dump(species_map, f)
    
    st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼å·²ç”Ÿæˆrfc_model.pklå’Œoutput_uniques.pkl")

# ===================== é¡µé¢åŠŸèƒ½ =====================
def intro_page():
    """ç®€ä»‹é¡µé¢ï¼ˆé€‚é…å®é™…åˆ—åï¼‰"""
    st.title("ä¼é¹…åˆ†ç±»å™¨ ğŸ§")
    st.header("æ•°æ®é›†ä»‹ç»ï¼ˆä¸­æ–‡ç‰ˆï¼‰")
    st.markdown("""**å¸•å°”é»˜ç¾¤å²›ä¼é¹…ä¸­æ–‡æ•°æ®é›†åŒ…å«344æ¡è§‚æµ‹è®°å½•ï¼Œæ¶µç›–3ç§å—æä¼é¹…ï¼š
    é˜¿å¾·åˆ©ä¼é¹…ã€å·´å¸ƒäºšä¼é¹…å’Œå¸½å¸¦ä¼é¹…ã€‚æ•°æ®è®°å½•äº†ä¼é¹…çš„æ –æ¯å²›å±¿ã€æ€§åˆ«ã€
    å–™é•¿åº¦/æ·±åº¦ã€ç¿…è†€é•¿åº¦åŠèº«ä½“è´¨é‡ç­‰å…³é”®ç‰¹å¾ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ åˆ†ç±»ç»ƒä¹ å’Œæ•°æ®å¯è§†åŒ–åˆ†æã€‚**""")
    
    st.header("ä¸‰ç§ä¼é¹…ç‰¹å¾å·®å¼‚")
    st.markdown("""
    - **é˜¿å¾·åˆ©ä¼é¹…**ï¼šä½“å‹è¾ƒå°ï¼Œå–™çŸ­è€Œé’ï¼Œä¸»è¦æ –æ¯äºæ‰˜æ‰˜å°”æ£®å²›ï¼›
    - **å·´å¸ƒäºšä¼é¹…**ï¼šä½“å‹è¾ƒå¤§ï¼Œå–™å°–ä¸”é•¿ï¼Œç¿…è†€é•¿åº¦æœ€é•¿ï¼›
    - **å¸½å¸¦ä¼é¹…**ï¼šå¤´éƒ¨æœ‰é»‘è‰²æ¡çº¹ï¼ˆä¼¼å¸½å¸¦ï¼‰ï¼Œå–™ä¸­ç­‰é•¿åº¦ã€‚
    """)
    
    # è¯»å–zipæ ¹ç›®å½•çš„penguins.png
    penguin_img = load_image_from_zip(ZIP_IMAGE_PATH, "penguins.png")
    if penguin_img:
        st.image(penguin_img, caption="ä¸‰ç§ä¼é¹…å¡é€šå›¾")
    else:
        st.info("âš ï¸ æœªåŠ è½½åˆ°ä¼é¹…ç¤ºæ„å›¾ï¼Œå¯æ£€æŸ¥images.zipå†…æ˜¯å¦æœ‰penguins.png")

def predict_page():
    """é¢„æµ‹é¡µé¢ï¼ˆä¿®å¤ç‰©ç§æ˜ å°„+ç‰¹å¾åˆ—åé—®é¢˜ï¼‰"""
    st.header("ä¼é¹…ç‰©ç§é¢„æµ‹")
    st.markdown("""è¾“å…¥ä»¥ä¸‹6é¡¹ä¼é¹…ç‰¹å¾ï¼Œç³»ç»Ÿå°†åŸºäºéšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å…¶ç‰©ç§ï¼š
    - æ³¨ï¼šç‰¹å¾å€¼éœ€ç¬¦åˆå®é™…èŒƒå›´ï¼ˆå¦‚å–™é•¿åº¦30-60mmï¼Œèº«ä½“è´¨é‡2700-6300gï¼‰""")
    
    # 3:1:2åˆ—å¸ƒå±€
    col_form, col1, col_logo = st.columns([3, 1, 2])
    with col_form:
        with st.form('user_inputs'):
            # ä¸­æ–‡è¾“å…¥è¡¨å•ï¼ˆä¸å®é™…åˆ†ç±»å€¼ä¸€è‡´ï¼‰
            island = st.selectbox('æ –æ¯å²›å±¿', options=['æ‰˜æ‰˜å°”æ£®å²›', 'æ¯”æ–¯ç§‘å²›', 'å¾·é‡Œå§†å²›'])
            sex = st.selectbox('æ€§åˆ«', options=['é›Œæ€§', 'é›„æ€§'])
            bill_length = st.number_input('å–™çš„é•¿åº¦(æ¯«ç±³)', min_value=30.0, max_value=60.0, value=45.0)
            bill_depth = st.number_input('å–™çš„æ·±åº¦(æ¯«ç±³)', min_value=15.0, max_value=25.0, value=20.0)
            flipper_length = st.number_input('ç¿…è†€çš„é•¿åº¦(æ¯«ç±³)', min_value=170.0, max_value=240.0, value=200.0)
            body_mass = st.number_input('èº«ä½“è´¨é‡(å…‹)', min_value=2700.0, max_value=6300.0, value=4000.0)
            submitted = st.form_submit_button('é¢„æµ‹ç‰©ç§', type='primary')

            # 1. åˆå§‹åŒ–ç‰¹å¾å‘é‡ï¼ˆä¸FEATURE_NAMESå¯¹é½ï¼Œæµ®ç‚¹å‹ï¼‰
            feature_vec = [0.0] * len(FEATURE_NAMES)
            # å¡«å……æ•°å€¼ç‰¹å¾ï¼ˆé€‚é…å®é™…åˆ—åï¼‰
            feature_vec[FEATURE_NAMES.index("å–™çš„é•¿åº¦")] = bill_length
            feature_vec[FEATURE_NAMES.index("å–™çš„æ·±åº¦")] = bill_depth
            feature_vec[FEATURE_NAMES.index("ç¿…è†€çš„é•¿åº¦")] = flipper_length
            feature_vec[FEATURE_NAMES.index("èº«ä½“è´¨é‡")] = body_mass
            # å¡«å……åˆ†ç±»ç‰¹å¾ï¼ˆç‹¬çƒ­ç¼–ç ï¼Œé€‚é…å®é™…åˆ—åï¼‰
            feature_vec[FEATURE_NAMES.index(f"ä¼é¹…æ –æ¯çš„å²›å±¿_{island}")] = 1.0
            feature_vec[FEATURE_NAMES.index(f"æ€§åˆ«_{sex}")] = 1.0

            # 2. é¢„æµ‹é€»è¾‘
            pred_species = ""  # åˆå§‹åŒ–é¢„æµ‹ç»“æœ
            if submitted:
                try:
                    # å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆç¡®ä¿ç”¨æœ€æ–°æ•°æ®ï¼‰
                    train_model(force_retrain=True)

                    # åŠ è½½æ¨¡å‹å’Œç‰©ç§æ˜ å°„
                    with open(MODEL_PATH, 'rb') as f:
                        rfc_model = pickle.load(f)
                    with open(MAP_PATH, 'rb') as f:
                        species_map = pickle.load(f)

                    # æ ¼å¼åŒ–è¾“å…¥æ•°æ®ï¼ˆç¡®ä¿åˆ—åã€ç±»å‹åŒ¹é…ï¼‰
                    input_df = pd.DataFrame(
                        data=[feature_vec],
                        columns=FEATURE_NAMES,
                        dtype=float
                    )

                    # é¢„æµ‹ï¼ˆè¿”å›ç¼–ç åçš„ç‰©ç§ç´¢å¼•ï¼‰
                    pred_idx = rfc_model.predict(input_df)[0]
                    # æ˜ å°„ç´¢å¼•åˆ°ç‰©ç§åï¼ˆç¡®ä¿species_mapçš„é”®æ˜¯æ•´æ•°ï¼‰
                    pred_species = species_map.get(int(pred_idx), "æœªçŸ¥ç‰©ç§")

                    # æ˜¾ç¤ºç»“æœ
                    if pred_species != "æœªçŸ¥ç‰©ç§":
                        st.success(f"ğŸ‰ é¢„æµ‹ç»“æœï¼šè¯¥ä¼é¹…ä¸º **{pred_species}**")
                    else:
                        st.warning("âš ï¸ æ— æ³•è¯†åˆ«è¯¥ä¼é¹…ç‰©ç§ï¼Œè¯·æ£€æŸ¥è¾“å…¥ç‰¹å¾æ˜¯å¦åˆç†")

                # ç»†åŒ–å¼‚å¸¸æ•è·
                except FileNotFoundError as e:
                    st.error(f"âŒ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼š{str(e)}")
                    st.info("æ­£åœ¨è‡ªåŠ¨é‡æ–°è®­ç»ƒæ¨¡å‹...")
                    train_model(force_retrain=True)
                except KeyError as e:
                    st.error(f"âŒ ç‰©ç§æ˜ å°„ä¸åŒ¹é…ï¼š{str(e)}")
                    st.info("å·²é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå†æ¬¡ç‚¹å‡»é¢„æµ‹æŒ‰é’®å³å¯")
                    train_model(force_retrain=True)
                except ValueError as e:
                    st.error(f"âŒ è¾“å…¥å€¼é”™è¯¯ï¼š{str(e)}")
                    st.info("è¯·ç¡®ä¿æ‰€æœ‰è¾“å…¥å€¼åœ¨åˆç†èŒƒå›´å†…ï¼ˆå¦‚èº«ä½“è´¨é‡2700-6300gï¼‰")
                except Exception as e:
                    st.error(f"âŒ é¢„æµ‹å‡ºé”™ï¼š{str(e)}")
                    st.info("å·²è‡ªåŠ¨ä¿®å¤æ¨¡å‹ï¼Œè¯·å†æ¬¡ç‚¹å‡»é¢„æµ‹æŒ‰é’®")
                    train_model(force_retrain=True)

    # å³ä¾§æ˜¾ç¤ºå›¾ç‰‡ï¼ˆä»zipæ ¹ç›®å½•è¯»å–ï¼‰
    with col_logo:
        if not submitted:
            # è¯»å–zipæ ¹ç›®å½•çš„logoå›¾ç‰‡
            logo_img = load_image_from_zip(ZIP_IMAGE_PATH, "rigth_logo.png")
            if logo_img:
                st.image(logo_img, width=300, caption="ä¼é¹…åˆ†ç±»å™¨")
            else:
                st.info("âš ï¸ æœªåŠ è½½åˆ°Logoå›¾ç‰‡ï¼Œå¯æ£€æŸ¥images.zipå†…æ˜¯å¦æœ‰rigth_logo.png")
        else:
            # é¢„æµ‹åè¯»å–å¯¹åº”ç‰©ç§å›¾ç‰‡
            if pred_species and pred_species != "æœªçŸ¥ç‰©ç§":
                species_img = load_image_from_zip(ZIP_IMAGE_PATH, f"{pred_species}.png")
                if species_img:
                    st.image(species_img, width=300, caption=f"{pred_species}")
                else:
                    st.info(f"âš ï¸ æœªåŠ è½½åˆ°{pred_species}çš„å›¾ç‰‡ï¼Œå¯æ£€æŸ¥images.zipå†…æ˜¯å¦æœ‰{pred_species}.png")
            else:
                st.info("âš ï¸ æš‚æ— æœ‰æ•ˆé¢„æµ‹ç»“æœï¼Œæ— æ³•åŠ è½½ç‰©ç§å›¾ç‰‡")

# ===================== ä¸»ç¨‹åº =====================
def main():
    # é¡µé¢åŸºç¡€é…ç½®
    st.set_page_config(
        page_title="ä¼é¹…åˆ†ç±»å™¨ï¼ˆä¸­æ–‡æ•°æ®é›†ç‰ˆï¼‰",
        page_icon="ğŸ§",
        layout="wide"
    )

    # ä¾§è¾¹æ å¯¼èˆªï¼ˆè¯»å–zipæ ¹ç›®å½•çš„logoï¼‰
    with st.sidebar:
        # è¯»å–zipæ ¹ç›®å½•çš„logoå›¾ç‰‡
        sidebar_logo = load_image_from_zip(ZIP_IMAGE_PATH, "rigth_logo.png")
        if sidebar_logo:
            st.image(sidebar_logo, width=100)
        st.title('åŠŸèƒ½å¯¼èˆª')
        page = st.selectbox(
            "é€‰æ‹©é¡µé¢", 
            ["ç®€ä»‹é¡µé¢", "é¢„æµ‹åˆ†ç±»é¡µé¢"], 
            label_visibility='collapsed'
        )

    # é¡µé¢è·¯ç”±
    if page == "ç®€ä»‹é¡µé¢":
        intro_page()
    else:
        predict_page()

if __name__ == "__main__":
    main()
